{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "print(\"Number of CPUs in the system:\", n_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.policy import default\n",
    "from gettext import install\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "import sklearn as sl\n",
    "#\n",
    "from ModelFitCV import ModelTrainCV, PrintTrainCVScores, PrintListScoresTrainCV\n",
    "from statistics import mean, stdev\n",
    "from numpy import mean\n",
    "from sklearn import preprocessing,tree \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, precision_score, recall_score, f1_score, precision_recall_fscore_support, roc_curve, roc_auc_score, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, classification_report,PrecisionRecallDisplay\n",
    "# splitsen dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# ML algorithme\n",
    "from sklearn.ensemble import RandomForestClassifier, Dec\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import plot_tree, DecisionTreeClassifier\n",
    "# scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_precision_recall_curve, PrecisionRecallDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve, RocCurveDisplay\n",
    "# plotten\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# feature selectie\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dfFeaturesTGO_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Formulate X en Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features=dataset.loc[:, dataset.columns!='label']     #selecteren van alle kolommen behalve label-kolom in alle rijen(:)\n",
    "Y_label=dataset['label']\n",
    "print(list(X_features.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scaling of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling for input features.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_scaled = scaler.fit_transform(X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, Y_label, test_size=0.2, random_state=0, stratify=Y_label)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating the classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Stratified Cross Validation & Fitting of model after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state=0,shuffle=True)\n",
    "\n",
    "f1_baseline_fold_sc, accuracy_baseline_fold_sc, spec_baseline_fold_sc, sens_baseline_sc, dt_model= ModelTrainCV(dt_model, skf, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **F1-score and specificity after scaling:** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintTrainCVScores(f1_baseline_fold_sc, accuracy_baseline_fold_sc, spec_baseline_fold_sc, sens_baseline_fold_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Feature selection - RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "#feature_selector = RFECV(estimator, step=5, cv=skf, n_jobs=n_cpu-1)\n",
    "#feature_selector = feature_selector.fit(x_train, y_train)\n",
    "#print(\"Optimal number of features : %d\" %feature_selector.n_features_)\n",
    "\n",
    "#feature_selector.ranking_\n",
    "#print('Best features: ', x_features[feature_selector.support_])\n",
    "#print('Best ranking', x_features[feature_selector.ranking_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a new x variable with the selected features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_selected_features = feature_selector.get_support(1)\n",
    "#x_selected_features = x_train[:, x_selected_features]\n",
    "\n",
    "#lijst = X_features.columns[feature_selector.support_] \n",
    "#print(lijst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_selected_features = x_train[:, [ 2,  3,  4,  5,  6,  8,  9, 10, 12, 13, 14, 32, 36, 37, 38, 40, 41,\n",
    "       43, 45, 46, 51, 53, 57, 59, 62]]\n",
    "\n",
    "x_test_selected_features = x_test[:, [ 2,  3,  4,  5,  6,  8,  9, 10, 12, 13, 14, 32, 36, 37, 38, 40, 41,\n",
    "       43, 45, 46, 51, 53, 57, 59, 62]]\n",
    "\n",
    "feature_selector_suport = ([False, False,  True,  True,  True,  True,  True, False,  True,\n",
    "        True,  True, False,  True,  True,  True, False, False, False,\n",
    "       False, False, False, False, False, False, False, False, False,\n",
    "       False, False, False, False, False,  True, False, False, False,\n",
    "        True,  True,  True, False,  True,  True, False,  True, False,\n",
    "        True,  True, False, False, False, False,  True, False,  True,\n",
    "       False, False, False,  True, False,  True, False, False,  True,\n",
    "       False, False, False])\n",
    "\n",
    "lijst = X_features.columns[feature_selector_suport]\n",
    "print(lijst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **training the model with the selected features and evaluating the new peformance:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rfe_fold_sc, accuracy_rfe_fold_sc, spec_rfe_fold_sc, sens_rfe_fold_sc, dt_model= ModelTrainCV(dt_model, skf, x_selected_features,y_train)\n",
    "\n",
    "PrintTrainCVScores(f1_rfe_fold_sc, accuracy_rfe_fold_sc, spec_rfe_fold_sc, sens_rfe_fold_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Tuning hyperparameters - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu=8\n",
    "param = {'criterion' :['entropy','gini'],'max_depth': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],'splitter':['best','random'], 'min_samples_leaf': [50]}\n",
    "Grid_Search = GridSearchCV(estimator=DecisionTreeClassifier(),param_grid= param, cv=skf, n_jobs=n_cpu-1)\n",
    "#gs_cv.fit(X_scaled,Y_label)\n",
    "Grid_Search.fit(x_selected_features, y_train)\n",
    "Grid_Search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Printing the results of the GridSearch:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max depth op 8 gezet, geef ongeveer zelfde accuracy. Voorkomt overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = Grid_Search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **training the model with hyperparameter tuning and evaluating the new peformance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skf = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "f1_GS_fold_sc, accuracy_GS_fold_sc, spec_GS_fold_sc, sens_GS_fold_sc, dt_model = ModelTrainCV(dt_model, skf, x_selected_features,y_train)\n",
    "PrintTrainCVScores(f1_GS_fold_sc, accuracy_GS_fold_sc, spec_GS_fold_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Scores for different max depth, on train and validation-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traind, x_testd, y_traind, y_testd = train_test_split(x_selected_features, y_train, test_size=0.2, random_state=0)\n",
    "y_testdd=y_testd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listF1=[]\n",
    "listse=[]\n",
    "listsp=[]\n",
    "listaccuracy=[]\n",
    "listF1_test=[]\n",
    "listsp_test=[]\n",
    "listse_test=[]\n",
    "listaccuracy_test=[]\n",
    "#skf = StratifiedKFold(n_splits=10,random_state=None,shuffle=False)\n",
    "\n",
    "for r in range(5,30,1):\n",
    "    model2=DecisionTreeClassifier(criterion=Grid_Search.best_estimator_.criterion, max_depth=r, splitter=Grid_Search.best_estimator_.splitter)\n",
    "    #skf = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "    f1fold_sc, accuracy_foldgrid, spec_foldgrid, sens_foldgrid = ModelTrainCV(model2, skf, x_traind,y_traind)\n",
    "\n",
    "    listF1.append((mean(f1fold_sc)))\n",
    "    listaccuracy.append((mean(accuracy_foldgrid)))\n",
    "    listsp.append((mean(spec_foldgrid)))\n",
    "    listse.append((mean(sens_foldgrid)))\n",
    "\n",
    "    f1fold_sc.clear()\n",
    "    accuracy_foldgrid.clear()\n",
    "    spec_foldgrid.clear()\n",
    "    sens_foldgrid.clear()\n",
    "    \n",
    "    predYd=model2.predict(x_testd)\n",
    "    f1d = f1_score(y_testdd,predYd)\n",
    "    listF1_test.append((f1d))\n",
    "    accuracyd = model2.score(x_testd,y_testd)\n",
    "    listaccuracy_test.append(accuracyd)\n",
    "    recall_specd = recall_score(y_testdd,predYd,pos_label=0)\n",
    "    listsp_test.append(recall_specd)\n",
    "    recall_sensd = recall_score(y_testdd,predYd,pos_label=1)\n",
    "    listse_test.append(recall_sensd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintListScoresTrainCV('f1_train', listF1, False)\n",
    "PrintListScoresTrainCV('spec_train', listsp, True)\n",
    "PrintListScoresTrainCV('sens_train', listse, True)\n",
    "\n",
    "\n",
    "PrintListScoresTrainCV('f1_test', listF1_test, False)\n",
    "PrintListScoresTrainCV('spec_test', listsp_test, True)\n",
    "PrintListScoresTrainCV('sens_test', listse_test, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdepth=np.arange(5,30,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot F1 vs max depth:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(30,5))\n",
    "fig.suptitle('F1 score for different depths of the Decision Tree')\n",
    "ax1.plot(xdepth,listF1, '-o', label='Train')\n",
    "ax1.plot(xdepth, listF1_test, 'r-o', label='Test')\n",
    "ax1.set_xlabel('Depth of decision tree')\n",
    "ax1.set_ylabel('F1 score')\n",
    "ax1.legend()\n",
    "ax2.plot(xdepth,listF1, '-o', label='Train')\n",
    "ax2.set_xlabel('Depth of decision tree')\n",
    "ax2.set_ylabel('F1 score')\n",
    "ax2.legend()\n",
    "ax3.plot(xdepth, listF1_test,'r-o', label='Test')\n",
    "ax3.set_xlabel('Depth of decision tree')\n",
    "ax3.set_ylabel('F1 score')\n",
    "ax3.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Plot specificity vs max depth:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(30,5))\n",
    "fig.suptitle('Specificity score for different depths of the Decision Tree')\n",
    "ax1.plot(xdepth,listsp, '-o', label='Train')\n",
    "ax1.plot(xdepth, listsp_test, 'r-o', label='Test')\n",
    "ax1.set_xlabel('Depth of decision tree')\n",
    "ax1.set_ylabel('Specificity')\n",
    "ax1.legend()\n",
    "ax2.plot(xdepth,listsp, '-o', label='Train')\n",
    "ax2.set_xlabel('Depth of decision tree')\n",
    "ax2.set_ylabel('Specificity')\n",
    "ax2.legend()\n",
    "ax3.plot(xdepth, listsp_test,'r-o', label='Test')\n",
    "ax3.set_xlabel('Depth of decision tree')\n",
    "ax3.set_ylabel('Specificity')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot sensitivity vs max depth:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(30,5))\n",
    "fig.suptitle('Sensitivity for different depths of the Decision Tree')\n",
    "ax1.plot(xdepth,listse, '-o', label='Train')\n",
    "ax1.plot(xdepth, listse_test, 'r-o', label='Test')\n",
    "ax1.set_xlabel('Depth of decision tree')\n",
    "ax1.set_ylabel('Sensitivity')\n",
    "ax1.legend()\n",
    "ax2.plot(xdepth,listse, '-o', label='Train')\n",
    "ax2.set_xlabel('Depth of decision tree')\n",
    "ax2.set_ylabel('Sensitivity')\n",
    "ax2.legend()\n",
    "ax3.plot(xdepth, listse_test,'r-o', label='Test')\n",
    "ax3.set_xlabel('Depth of decision tree')\n",
    "ax3.set_ylabel('Sensitivity')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot accuracy vs max depth:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(30,5))\n",
    "fig.suptitle('Accuracy for different depths of the Decision Tree')\n",
    "ax1.plot(xdepth,listaccuracy, '-o', label='Train')\n",
    "ax1.plot(xdepth, listaccuracy_test, 'r-o', label='Test')\n",
    "ax1.set_xlabel('Depth of decision tree')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax2.plot(xdepth,listaccuracy, '-o', label='Train')\n",
    "ax2.set_xlabel('Depth of decision tree')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax3.plot(xdepth, listaccuracy_test,'r-o', label='Test')\n",
    "ax3.set_xlabel('Depth of decision tree')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Scores for different min_samples_leaf, on train and validation-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listF1=[]\n",
    "listse=[]\n",
    "listsp=[]\n",
    "listaccuracy=[]\n",
    "listF1_test=[]\n",
    "listsp_test=[]\n",
    "listse_test=[]\n",
    "listaccuracy_test=[]\n",
    "#skf = StratifiedKFold(n_splits=10,random_state=None,shuffle=False)\n",
    "\n",
    "for r in range(10,110,10):\n",
    "    model2=DecisionTreeClassifier(criterion=Grid_Search.best_estimator_.criterion, max_depth=Grid_Search.best_estimator_.max_depth, min_samples_leaf=r,splitter=Grid_Search.best_estimator_.splitter)\n",
    "    f1fold_sc, accuracy_foldgrid, spec_foldgrid, sens_foldgrid = ModelTrainCV(model2, skf, x_traind,y_traind)\n",
    "\n",
    "    listF1.append((mean(f1fold_sc)))\n",
    "    listaccuracy.append((mean(accuracy_foldgrid)))\n",
    "    listsp.append((mean(spec_foldgrid)))\n",
    "    listse.append((mean(sens_foldgrid)))\n",
    "\n",
    "    f1fold_sc.clear()\n",
    "    accuracy_foldgrid.clear()\n",
    "    spec_foldgrid.clear()\n",
    "    sens_foldgrid.clear()\n",
    "    \n",
    "    predYd=model2.predict(x_testd)\n",
    "    f1d = f1_score(y_testdd,predYd)\n",
    "    listF1_test.append((f1d))\n",
    "    accuracyd = model2.score(x_testd,y_testd)\n",
    "    listaccuracy_test.append(accuracyd)\n",
    "    recall_specd = recall_score(y_testdd,predYd,pos_label=0)\n",
    "    listsp_test.append(recall_specd)\n",
    "    recall_sensd = recall_score(y_testdd,predYd,pos_label=1)\n",
    "    listse_test.append(recall_sensd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xminsamplesleaf=np.arange(10,110,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot F1 vs min_samples_leaf:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(30,5))\n",
    "fig.suptitle('F1 score for different min_samples_leaf of the Decision Tree')\n",
    "ax1.plot(xminsamplesleaf,listF1, '-o', label='Train')\n",
    "ax1.plot(xminsamplesleaf, listF1_test, 'r-o', label='Test')\n",
    "ax1.set_xlabel('Min_samples_leaf of decision tree')\n",
    "ax1.set_ylabel('F1 score')\n",
    "ax1.legend()\n",
    "ax2.plot(xminsamplesleaf,listF1, '-o', label='Train')\n",
    "ax2.set_xlabel('Min_samples_leaf of decision tree')\n",
    "ax2.set_ylabel('F1 score')\n",
    "ax2.legend()\n",
    "ax3.plot(xminsamplesleaf, listF1_test,'r-o', label='Test')\n",
    "ax3.set_xlabel('Min_samples_leaf of decision tree')\n",
    "ax3.set_ylabel('F1 score')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot specificity vs min_samples_leaf:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(30,5))\n",
    "fig.suptitle('Specificity score for different min_samples_leaf of the Decision Tree')\n",
    "ax1.plot(xminsamplesleaf,listsp, '-o', label='Train')\n",
    "ax1.plot(xminsamplesleaf, listsp_test, 'r-o', label='Test')\n",
    "ax1.set_xlabel('Min_samples_leaf of decision tree')\n",
    "ax1.set_ylabel('Specificity')\n",
    "ax1.legend()\n",
    "ax2.plot(xminsamplesleaf,listsp, '-o', label='Train')\n",
    "ax2.set_xlabel('Min_samples_leaf of decision tree')\n",
    "ax2.set_ylabel('Specificity')\n",
    "ax2.legend()\n",
    "ax3.plot(xminsamplesleaf, listsp_test,'r-o', label='Test')\n",
    "ax3.set_xlabel('Min_samples_leaf of decision tree')\n",
    "ax3.set_ylabel('Specificity')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot accuracy vs min_samples_leaf:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(30,5))\n",
    "fig.suptitle('Accuracy for different min_samples_leaf of the Decision Tree')\n",
    "ax1.plot(xminsamplesleaf,listaccuracy, '-o', label='Train')\n",
    "ax1.plot(xminsamplesleaf, listaccuracy_test, 'r-o', label='Test')\n",
    "ax1.set_xlabel('min_samples_leaf of decision tree')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax2.plot(xminsamplesleaf,listaccuracy, '-o', label='Train')\n",
    "ax2.set_xlabel('min_samples_leaf of decision tree')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax3.plot(xminsamplesleaf, listaccuracy_test,'r-o', label='Test')\n",
    "ax3.set_xlabel('min_samples_leaf of decision tree')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting the Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,30))\n",
    "tree.plot_tree(dt_model, max_depth=4, feature_names=lijst, class_names=['non-epileptic', 'epileptic'], label='all', impurity=True,proportion=True, rounded=True, precision=3)\n",
    "plt.show()\n",
    "fig.savefig('decision_tree.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature importance as given by the Decission Tree model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importances = dt_model.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=lijst)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar( ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plot giving better insight in Decission Tree decission making:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtreeviz.trees import dtreeviz # remember to load the package\n",
    "\n",
    "viz = dtreeviz(dt_model, x_selected_features, y_train,\n",
    "                 depth_range_to_display=(0,4), precision=6\n",
    ")\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Evaluation of Decission Tree model <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = dt_model.predict(x_test_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_Y)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['non-epileptic', 'epileptic/removed'])\n",
    "disp.plot(cmap = 'GnBu')\n",
    "disp.ax_.set_title(\"Confusion Matrix for epilepsy dataset\")\n",
    "#[TN,FP] \n",
    "#[FN,TP]\n",
    "\n",
    "precision = precision_score(y_test,pred_Y) #TP/(TP+FP)\n",
    "recall_sens = recall_score(y_test,pred_Y,pos_label=1) #sensitivity; TP/(TP+FN)\n",
    "recall_spec = recall_score(y_test,pred_Y,pos_label=0) #TN/(TN+FP)\n",
    "accuracy = accuracy_score(y_test,pred_Y) #(TN+TP)/(TN+FP+FN+TP)\n",
    "f1score = f1_score(y_test,pred_Y) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('\\n precision {0:.6f}'. format(precision), '\\n sensitiviteit {0:.6f}'. format(recall_sens),'\\n specificiteit {0:.6f}'. format(recall_spec), '\\n accuracy {0:.6f}'. format(accuracy), '\\n f1 score {0:.6f}'. format(f1score))\n",
    "\n",
    "LR_pos = recall_sens / (1-recall_spec) #>1.0\n",
    "LR_neg = (1-recall_sens) / recall_spec #<1.0\n",
    "print('Positive likelihood ratio', LR_pos, '\\nNegative likelihood ratio', LR_neg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification Report:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClassification report: \\n', format(classification_report(y_test, pred_Y,target_names=['non-epileptic','epileptic/removed'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ROC curve:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(dt_model, x_test_selected_features, y_test)\n",
    "plt.plot([0,1], [0,1], 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Precision recall curve:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PrecisionRecallDisplay.from_estimator(lr_model, selection, y_train)\n",
    "plot_precision_recall_curve(dt_model, x_test_selected_features, y_test)\n",
    "plt.show()\n",
    "\n",
    "# from predictions\n",
    "PRD_pred = PrecisionRecallDisplay(y_test, pred_Y)\n",
    "PRD_pred.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Randomly changing epileptic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = [1, 2, 3, 4, 5]\n",
    "percentage = [(.05, .95), (.1, .90), (.15, .85), (.2, .8), (.25, .75), (.3, .7), (.35, .65), (.4, .6), (.45, .55), (.5, 0.5)]\n",
    "\n",
    "f1 = []\n",
    "spec = []\n",
    "sens =[]\n",
    "acc = []\n",
    "\n",
    "for p in percentage:\n",
    "    print(p)\n",
    "\n",
    "    for r in range(1,6):\n",
    "        y_series = pd.Series(y_train)\n",
    "        df_y_train = y_series.to_frame()\n",
    "        print((df_y_train.label==1).sum()) \n",
    "        epileptic_obs = df_y_train.label == 1\n",
    "        df_y_train.loc[epileptic_obs, 'label'] = np.random.choice((0,1), epileptic_obs.sum(), p=(p))\n",
    "        print((df_y_train.label==1).sum())\n",
    "\n",
    "        model = Grid_Search.best_estimator_\n",
    "\n",
    "        f1_fold_sc, accuracy_fold_sc, spec_fold_sc, sens_fold_sc, lr2_model = ModelTrainCV(model, skf, x_selected_features, df_y_train)\n",
    "\n",
    "        #PrintTrainCVScores(f1_fold_sc, accuracy_fold_sc, spec_fold_sc)\n",
    "       \n",
    "        pred_Y = lr2_model.predict(x_test_selected_features)\n",
    "\n",
    "        recall_sens1 = recall_score(y_test,pred_Y,pos_label=1) #sensitivity; TP/(TP+FN)\n",
    "        recall_spec1 = recall_score(y_test,pred_Y,pos_label=0) #TN/(TN+FP)\n",
    "        accuracy1 = accuracy_score(y_test,pred_Y) #(TN+TP)/(TN+FP+FN+TP)\n",
    "        f1score2 = f1_score(y_test,pred_Y) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        sens.append('{0:.1f}%'. format((recall_sens1)*100))\n",
    "        spec.append('{0:.1f}%'. format((recall_spec1)*100))\n",
    "        acc.append('{0:.1f}%'.format((accuracy1)*100))\n",
    "        f1.append('{0:.3f}'. format(f1score2))\n",
    "\n",
    "        #print('\\n sensitiviteit {0:.1f}%'. format((recall_sens)*100),'\\n specificiteit {0:.1f}'. format((recall_spec)*100), '\\n accuracy {0:.3f}'.format((accuracy)*100), '\\n f1 score {0:.3f}'. format(f1score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_perc = [0.598,.573,.548,.528,.495,.45,.407,.318,.287,.218]\n",
    "acc_perc = [76.3,76.4,75.6,75.2,74.9,73.7,73.1,72.4,71.4,70.1]\n",
    "spec_perc = [87.5,89.7,90.7,91.7,92.9,95.3,96.2,97.3,97.5,98.4]\n",
    "sens_perc = [53.2,49.3,45.2,41.8,38.3,29.7,26.2,22,18.6,12.7]\n",
    "perc = [5, 10, 15,20,25,30,35,40,45,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=4, sharex=True, figsize=(20,5))\n",
    "fig.suptitle('DT Model scores for randomly changing percentages of the labelled 1 observations')\n",
    "\n",
    "ax.set_ylim(0,1)\n",
    "ax.plot(perc, f1_perc, 'b-o', label='f1-score')\n",
    "ax.hlines(y=f1score, xmin=0, xmax=50, linestyles='--', color='b', alpha=.5, label='f1 reference score')\n",
    "ax.set_xlabel('percentage of changed y_train labels')\n",
    "ax.set_ylabel('average f1-score')\n",
    "ax.legend(loc='upper right')   \n",
    "\n",
    "ax1.set_ylim(0, 105)\n",
    "ax1.plot(perc, acc_perc, 'g-o', label='accuracy')\n",
    "ax1.hlines(y=((accuracy)*100), xmin=0, xmax=50, linestyles='--', color='g', alpha=.5, label='f1 reference score')\n",
    "ax1.set_xlabel('percentage of changed y_train labels')\n",
    "ax1.set_ylabel('average accuracy score')\n",
    "ax1.legend(loc='lower right') \n",
    "\n",
    "ax2.set_ylim(0, 105)\n",
    "ax2.plot(perc, spec_perc, 'r-o', label='specificity')\n",
    "ax2.hlines(y=((recall_spec)*100), xmin=0, xmax=50, linestyles='--', color='r', alpha=.5, label='f1 reference score')\n",
    "ax2.set_xlabel('percentage of changed y_train labels')\n",
    "ax2.set_ylabel('average specificity score')\n",
    "ax2.legend(loc='lower right') \n",
    "\n",
    "ax3.set_ylim(0, 105)\n",
    "ax3.plot(perc, sens_perc, 'y-o', label='sensitivity')\n",
    "ax3.hlines(y=((recall_sens)*100), xmin=0, xmax=50, linestyles='--', color='y', alpha=.5, label='f1 reference score')\n",
    "ax3.set_xlabel('percentage of changed y_train labels')\n",
    "ax3.set_ylabel('average sensitivity score')\n",
    "ax3.legend(loc='upper right') \n",
    "\n",
    "fig.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('baseupdated')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4c40a720fe60b6922c6261686befa02cb82987a5a78cfa28fca5afd5a9c8dfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
