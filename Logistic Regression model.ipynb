{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "  \n",
    "n_cpu = os.cpu_count()\n",
    "print(\"Number of CPUs in the system:\", n_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "import sklearn as sl\n",
    "#\n",
    "from ModelFitCV import ModelTrainCV, PrintTrainCVScores, PrintListScoresTrainCV\n",
    "from statistics import mean, stdev\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn import preprocessing\n",
    "# splitsen dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "# ML algorithme\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_precision_recall_curve, PrecisionRecallDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix, ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve, RocCurveDisplay\n",
    "# plotten\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# feature selectie\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "print('All libraries are installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dfFeaturesTGO_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Formulate X and Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = dataset.drop(columns=['label'])\n",
    "Y_label=dataset['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scaling of the data <br>\n",
    "Standardization is the process of transforming data in a way such that the mean of each column becomes equal to zero, and the standard deviation of each column is one. This way, you obtain the same scale for all columns. Take the following steps to standardize your data: <br>\n",
    "Calculate the mean and standard deviation for each column. <br>\n",
    "Subtract the corresponding mean from each element. <br>\n",
    "Divide the obtained difference by the corresponding standard deviation. <br>\n",
    "\n",
    "By scaling the data of our dataset the measurements of the different features are directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "x_scaled = scaler.fit_transform(X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Splitting the data <br>\n",
    "First we create a training and a testing dataset. The training set will be used for training and validation of the model. With validation we mean that we evaluate the different settings of the hyperparameters on the estimations performed by the model. Afterwhich, we will perform a final evaluation of the model on the test set. <br>\n",
    "By setting the random state to an integer the split is reproducible for repeated use of the function. <br>\n",
    "By setting the stratify argument to Y_label component of the original dataset the train and the test set will have the same distribution of epileptic and non-epileptic observations as the original dataset. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, Y_label, test_size=0.2, random_state=0, stratify=Y_label)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating the classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(max_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Stratified Cross Validation & Fitting of model after scaling<br>\n",
    "Using stratisfied K-fold CrossValidation. This means that we will create k-folds of testing and training sets wihtin the previously created training set. This ensures that we do not have to use our testingdata for validating the model after evaluating the effect feature selection and tuning of the hyperparameters from the model.<br>\n",
    "\n",
    "With stratification we will keep the distribution of the epileptic (Label =1) and non-epileptic patients over the training and validation set. <br>\n",
    "\n",
    "By using the function .fit which describes the process of calculating the best weights using the available observations. The function calculates the best weights by maximizing the log-likelihood function(LLF) for all observations i.<br>\n",
    "We will first use this function for the model with default settings and using all available features to get a baseline for the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "f1_baseline_fold_sc, accuracy_baseline_fold_sc, spec_baseline_fold_sc, sens_baseline_fold_sc, lr_model = ModelTrainCV(lr_model, skf, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **F1-score and specificity after scaling:** <br>\n",
    "Baseline scores for performance of the model. Optimalizations are yet to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintTrainCVScores(f1_baseline_fold_sc, accuracy_baseline_fold_sc, spec_baseline_fold_sc, sens_baseline_fold_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Coefficients:** <br>\n",
    "Evaluating the coefficients given to each feature after training the model.<br> The coefficient value is a measure for the dependency of the outcome on the coresponding feature given that all other features remain constant (=conditional dependency); with a change of 1 unit of a feature the outcome will change with the coresponding coefficient. <br>\n",
    "This means that a large coefficient value for a specific feature means that this feature had a big impact on the outcome we're trying to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.intercept_\n",
    "\n",
    "lr_model.coef_\n",
    "\n",
    "coefficents=lr_model.coef_\n",
    "coefficents.shape\n",
    "coefficents=np.reshape(coefficents, 66)\n",
    "\n",
    "pd.DataFrame(coefficents, X_features.columns, columns=['coefficients']).sort_values(by='coefficients', ascending=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying coefficients bij the standard deviation of the related feature will reduce all coefficients to the same unit of measure which makes them directly comparable.<br>\n",
    "The figure below shows the coefficients given to the features from the dataset by the LogisticRegression model. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_features.columns\n",
    "coefficents=lr_model.coef_\n",
    "coefficents.shape\n",
    "coefficents=np.reshape(coefficents, 66)\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "   coefficents * x_train.std(axis=0),\n",
    "    columns=[\"Coefficient importance\"],\n",
    "    index=feature_names,\n",
    ").sort_values(by='Coefficient importance', ascending=True) \n",
    "\n",
    "coefs.plot(kind=\"barh\", figsize=((15, 20)), fontsize = 'x-large')\n",
    "plt.xlabel(\"Coefficient values corrected by the feature's std. dev.\", fontsize = 'x-large' )\n",
    "plt.title(\"Logist regresion model\", fontsize = 20)\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Feature Selection <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive Feature Elimination (RFE)** <br>\n",
    "This method of feature selection will select features by repeatedly evaluating the model and with each repeat the worst performing feature will be removed. This process will be repeated untill the desired number of features is selected. <br>\n",
    "The importance of each feature as giving by its coefficient, this is the measure used for determening the performance of the feature.\n",
    "\n",
    "RFECV is a method that performs RFE in a cross-validation loop to find the optimal number of features. This is the method that will be used for our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "#feature_selector = RFECV(estimator, step=1, cv=skf, n_jobs=n_cpu-2)\n",
    "#feature_selector = feature_selector.fit(x_train, y_train)\n",
    "#print(\"Optimal number of features : %d\" %feature_selector.n_features_)\n",
    "#feature_selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a new x variable with the selected features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_selected_features = feature_selector.get_support(1)\n",
    "#x_selected_features = x_train[:, x_selected_features]\n",
    "\n",
    "#lijst = X_features.columns[feature_selector.support_] \n",
    "#print(lijst)\n",
    "\n",
    "x_selected_features = x_train[:, [ 2,  3,  4,  5,  6,  8,  9, 10, 12, 13, 14, 32, 36, 37, 38, 40, 41,\n",
    "       43, 45, 46, 51, 53, 57, 59, 62]]\n",
    "\n",
    "x_test_selected_features = x_test[:, [ 2,  3,  4,  5,  6,  8,  9, 10, 12, 13, 14, 32, 36, 37, 38, 40, 41,\n",
    "       43, 45, 46, 51, 53, 57, 59, 62]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_selector.get_support(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **training the model with the selected features and evaluating the new peformance:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rfe_fold_sc, accuracy_rfe_fold_sc, spec_rfe_fold_sc, sens_rfe_fold_sc, lr_model = ModelTrainCV(lr_model, skf, x_selected_features,y_train)\n",
    "\n",
    "PrintTrainCVScores(f1_rfe_fold_sc, accuracy_rfe_fold_sc, spec_rfe_fold_sc, sens_rfe_fold_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Tuning hyperparameters <br>\n",
    "### **Create search space** <br>\n",
    "For logistic Regression models the following parameters were evaluated using GridSearch.\n",
    "- solver ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']:<br>\n",
    "    Algorithm which is used to solve the cost function\n",
    "- penalty ['none', 'l1', 'l2', 'elasticnet']: <br>\n",
    "    penalty term which is added to the cost funtion\n",
    "- C [100, 10, 1.0, 0.1, 0,01]: <br>\n",
    "    regularization parameter. For small values of C the regularization strength is higher. This means the model will be simple and is likely to underfit. For larg values of C, the regularization strengt is lower which leads to a more complex model. This can lead to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    { 'solver' : ['newton-cg', 'lbfgs', 'sag'], 'penalty' : ['l2', 'none'], 'C' : [1000, 100, 10, 1.0, 0.1, 0.01, 0.001]},\n",
    "    { 'solver' : ['liblinear'], 'penalty' : ['l1', 'l2'], 'C' : [1000, 100, 10, 1.0, 0.1, 0.01, 0.001]},\n",
    "    { 'solver' : ['saga'], 'penalty' : ['elasticnet', 'l1', 'l2', 'none'], 'C' : [1000, 100, 10, 1.0, 0.1, 0.01, 0.001]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run the GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting refit='AUC', refits an estimator on the whole dataset with the parameter setting that has the best cross-validated AUC score.\n",
    "#Grid_Search = GridSearchCV(estimator=LogisticRegression(max_iter=10000), param_grid=param_grid, cv=skf, verbose=0, n_jobs=6)\n",
    "#Grid_Search.fit(x_selected_features, y_train)\n",
    "#Grid_Search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Printing the results of the GridSearch per fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_model=LogisticRegression(solver=Grid_Search.best_estimator_.solver, penalty=Grid_Search.best_estimator_.penalty, C=Grid_Search.best_estimator_.C)\n",
    "#lr_model = Grid_Search.best_estimator_\n",
    "lr_model = LogisticRegression(C=1.0, penalty = 'l2', solver= 'saga', max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_GS_fold_sc, accuracy_GS_fold_sc, spec_GS_fold_sc, sens_GS_fold_sc, lr_model = ModelTrainCV(lr_model, skf, x_selected_features,y_train)\n",
    "PrintTrainCVScores(f1_GS_fold_sc, accuracy_GS_fold_sc, spec_GS_fold_sc, sens_GS_fold_sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Updated coefficients from LogisticRegression model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.intercept_\n",
    "feature_selector_suport = ([False, False,  True,  True,  True,  True,  True, False,  True,\n",
    "        True,  True, False,  True,  True,  True, False, False, False,\n",
    "       False, False, False, False, False, False, False, False, False,\n",
    "       False, False, False, False, False,  True, False, False, False,\n",
    "        True,  True,  True, False,  True,  True, False,  True, False,\n",
    "        True,  True, False, False, False, False,  True, False,  True,\n",
    "       False, False, False,  True, False,  True, False, False,  True,\n",
    "       False, False, False])\n",
    "\n",
    "coefficents=lr_model.coef_\n",
    "coefficents.shape\n",
    "coefficents=np.reshape(coefficents, 25)\n",
    "\n",
    "pd.DataFrame(coefficents, X_features.columns[feature_selector_suport], columns=['coefficients']).sort_values(by='coefficients', ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_features.columns[feature_selector_suport]\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "   coefficents * x_selected_features.std(axis=0),\n",
    "    columns=[\"Coefficient importance\"],\n",
    "    index=feature_names,\n",
    ").sort_values(by='Coefficient importance', ascending=True) \n",
    "\n",
    "coefs.plot(kind=\"barh\", figsize=((10, 15)), fontsize='x-large')\n",
    "plt.xlabel(\"Coefficient values corrected by the feature's std. dev.\")\n",
    "plt.title(\"Logist regresion model\", fontsize=20)\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio_coeff = np.exp(coefficents)\n",
    "\n",
    "coefs_OR = pd.DataFrame(\n",
    "   odds_ratio_coeff ,\n",
    "    columns=[\"odds_ratio\"],\n",
    "    index=feature_names,\n",
    ").sort_values(by='odds_ratio', ascending=True) \n",
    "\n",
    "coefs_OR.reset_index(inplace=True)\n",
    "coefs_OR = coefs_OR.rename(columns = {'index':'features'})\n",
    "\n",
    "coefs_OR.plot(kind=\"scatter\",marker=\"o\", s=70, x='odds_ratio', y='features', figsize=((10, 15)), fontsize='x-large')\n",
    "plt.xlabel(\"Odds ratio\", fontsize=16)\n",
    "plt.title(\"Odds ratio of features\", fontsize=20)\n",
    "plt.axvline(x=1, color=\"red\", alpha=.5)\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Evaluation Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(x_test_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation scores & Confusion matrix:** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['non-epileptic', 'removed/epileptic'])\n",
    "disp.plot(cmap = 'GnBu')\n",
    "disp.ax_.set_title(\"Confusion Matrix for epilepsy dataset\", fontsize=20)\n",
    "\n",
    "spec = cm[0,0]/(cm[0,0]+cm[0,1]) #TN/(TN+FP)\n",
    "recall = recall_score(y_test, y_pred) #sensitivity; TP/(TP+FN)\n",
    "LR_pos = recall / (1-spec) #>1.0\n",
    "LR_neg = (1-recall) / spec #<1.0\n",
    "print('Positive likelihood ratio', LR_pos,'\\nNegative likelihood ratio', LR_neg)\n",
    "\n",
    "precision = precision_score(y_test, y_pred) #TP/(TP+FP)\n",
    "recall_sens = recall_score(y_test,y_pred,pos_label=1) #sensitivity; TP/(TP+FN)\n",
    "recall_spec = recall_score(y_test,y_pred,pos_label=0) #TN/(TN+FP)\n",
    "accuracy_1 = accuracy_score(y_test,y_pred) #(TN+TP)/(TN+FP+FN+TP)\n",
    "f1score = f1_score(y_test,y_pred) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('\\n precision', precision, '\\n sensitiviteit', recall_sens,'\\n specificitity', recall_spec, '\\n accuracy', accuracy_1, '\\n f1 score', f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification report:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ROC curve:**\n",
    "This curve sumerizes the classification performance of the model for the positive classifications <br>\n",
    "**AUC** is the area under the curve <br>\n",
    "The x-as describes the false positive rate and the y-as describes the true positve rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(lr_model, x_test_selected_features, y_test)\n",
    "plt.plot([0, 1], [0, 1],'r--', label='No Skill') \n",
    "plt.xlabel()\n",
    "plt.ylabel() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Precision-Recall curve:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(lr_model, x_test_selected_features, y_test)\n",
    "plt.show(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the threshold value for classification: <br>\n",
    "We will test different threshold values: the only certainty of our dataset is that the negative observations are actually negative and that the EZ was removed. It is possible that some of the observations that were labeled positive may be measurements from healty brain tissue that was removed either as a precaussion margin, to get to the epileptic tissue of falsly classified as epileptic. <br>\n",
    "This is why we will focus on the threshold values that will minimizes the false positve rate or the specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_threshold = []\n",
    "spec_threshold = []\n",
    "f1_threshold = []\n",
    "sens_threshold = []\n",
    "\n",
    "pred_proba_df = pd.DataFrame(lr_model.predict_proba(x_test_selected_features))\n",
    "threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\n",
    "for i in threshold_list:\n",
    "    print ('\\n******** For i = {} ******'.format(i))\n",
    "    Y_test_pred = pred_proba_df.applymap(lambda x: 1 if x>i else 0)\n",
    "    test_accuracy = metrics.accuracy_score((y_test),\n",
    "                        Y_test_pred.iloc[:,[1]])\n",
    "    \n",
    "    print('Our testing accuracy is {:.1%}'.format((test_accuracy)))\n",
    "   \n",
    "    f1 = f1_score(y_test, Y_test_pred.iloc[:,[1]])\n",
    "    print('f1-score is {:.3f}'.format(f1))\n",
    "\n",
    "    specificity = recall_score(y_test, Y_test_pred.iloc[:,[1]], pos_label=0)\n",
    "    sensitivity = recall_score(y_test, Y_test_pred.iloc[:,[1]],pos_label=1)\n",
    "    print(\"specificity is {:.1f}%\".format((specificity)*100))\n",
    "    print(\"sensitivity is {:.1f}%\".format((sensitivity)*100))\n",
    "\n",
    "    acc_threshold.append('{:.1f}'.format((test_accuracy)*100))\n",
    "    spec_threshold.append('{:.1f}'.format((specificity)*100))\n",
    "    f1_threshold.append('{:.3f}'.format(f1))\n",
    "    sens_threshold.append('{:.1f}'.format((sensitivity)*100))\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix(y_test, Y_test_pred.iloc[:,[1]]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_threshold)\n",
    "print(spec_threshold)\n",
    "print(acc_threshold)\n",
    "print(sens_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_threshold = [0.506, 0.508, 0.514, 0.527, 0.542, 0.541, 0.526, 0.503, 0.439, 0.348, 0.255, 0.153, 0.085, 0.050, 0.021, 0.009, 0.006, 0.003, 0.001, 0.000]\n",
    "spec_threshold = [4.4, 5.8, 10.0, 21.6, 38.5, 54.8, 68.1, 78.8, 86.5, 92.1, 95.6, 97.7, 98.9, 99.6, 99.9, 99.9, 99.9, 99.9, 100.0, 100.0]\n",
    "acc_threshold = [35.8, 36.6, 39.0, 45.1, 53.4, 60.2, 65.0, 68.7, 69.8, 69.8, 69.3, 68.3, 67.7, 67.6, 67.3, 67.1, 67.0, 67.0, 67.0, 67.0]\n",
    "sens_threshold = [99.7, 99.2, 97.9, 92.8, 83.6, 71.1, 58.8, 48.1, 35.9, 24.4, 15.9, 8.7, 4.5, 2.6, 1.0, 0.5, 0.3, 0.1, 0.1, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=4, sharex=True, figsize=(20,5))\n",
    "fig.suptitle('Model scores for varing thresholds', fontsize=20)\n",
    "\n",
    "\n",
    "ax.set_ylim(0,1)\n",
    "#ax.invert_yaxis()\n",
    "ax.plot(threshold_list, f1_threshold, 'b-o', label='f1-score')\n",
    "ax.hlines(y=((f1score)), xmin=0, xmax=1, linestyles='--', color='b', alpha=.5, label='f1 reference score')\n",
    "ax.set_xlabel('thresholds', fontsize=16)\n",
    "ax.set_ylabel('average f1-score', fontsize=16)\n",
    "ax.legend(loc='lower right', fontsize=12)   \n",
    "\n",
    "ax1.set_ylim(0,105)\n",
    "ax1.plot(threshold_list, spec_threshold, 'r-o', label='specificity')\n",
    "ax1.hlines(y=(((recall_spec)*100)), xmin=0, xmax=1, linestyles='--', color='r', alpha=.5, label='specificity reference score')\n",
    "ax1.set_xlabel('thresholds', fontsize=16)\n",
    "ax1.set_ylabel('average specificity score', fontsize=16)\n",
    "ax1.legend(loc='lower right', fontsize=12)   \n",
    "\n",
    "ax2.set_ylim(0,105)\n",
    "ax2.plot(threshold_list, acc_threshold, 'g-o', label='accuracy')\n",
    "ax2.hlines(y=(((accuracy_1)*100)), xmin=0, xmax=1, linestyles='--', color='g', alpha=.5, label='accuracy reference score')\n",
    "ax2.set_xlabel('thresholds', fontsize=16)\n",
    "ax2.set_ylabel('average accuracy score', fontsize=16)\n",
    "ax2.legend(loc='lower right', fontsize=12)  \n",
    "\n",
    "ax3.set_ylim(0,105)\n",
    "ax3.plot(threshold_list, sens_threshold, 'y-o', label='specificity')\n",
    "ax3.hlines(y=(((recall_sens)*100)), xmin=0, xmax=1, linestyles='--', color='y', alpha=.5, label='specificity reference score')\n",
    "ax3.set_xlabel('thresholds', fontsize=16)\n",
    "ax3.set_ylabel('average sensitivity score', fontsize=16)\n",
    "ax3.legend(loc='lower right', fontsize=12)  \n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly changing epileptic labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly chaning a range of 5-50 percentage of the labelled 1 observations in the train set. Afterwhich, the model will be trained on the altered train set and re-evaluated on the unaltered test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = [1, 2, 3, 4, 5]\n",
    "percentage = [(.05, .95), (.1, .90), (.15, .85), (.2, .8), (.25, .75), (.3, .7), (.35, .65), (.4, .6), (.45, .55), (.5, 0.5)]\n",
    "\n",
    "f1 = []\n",
    "spec = []\n",
    "sens =[]\n",
    "acc = []\n",
    "\n",
    "for p in percentage:\n",
    "    print(p)\n",
    "\n",
    "    for r in range(1,6):\n",
    "        #y_series = pd.Series(y_train)\n",
    "        df_y_train = y_train.to_frame()\n",
    "        print((df_y_train.label==1).sum()) \n",
    "        epileptic_obs = df_y_train.label == 1\n",
    "        df_y_train.loc[epileptic_obs, 'label'] = np.random.choice((0,1), epileptic_obs.sum(), p=(p))\n",
    "        print((df_y_train.label==1).sum())\n",
    "\n",
    "        f1_fold_sc, accuracy_fold_sc, spec_fold_sc, sens_fold_sc, lr2_model = ModelTrainCV(lr_model, skf, x_selected_features, df_y_train)\n",
    "       \n",
    "        pred_Y = lr2_model.predict(x_test_selected_features)\n",
    "\n",
    "        recall_sens = recall_score(y_test,pred_Y,pos_label=1) #sensitivity; TP/(TP+FN)\n",
    "        recall_spec = recall_score(y_test,pred_Y,pos_label=0) #TN/(TN+FP)\n",
    "        accuracy = accuracy_score(y_test,pred_Y) #(TN+TP)/(TN+FP+FN+TP)\n",
    "        f1score2 = f1_score(y_test,pred_Y) #F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        sens.append('{0:.1f}%'. format((recall_sens)*100))\n",
    "        spec.append('{0:.1f}%'. format((recall_spec)*100))\n",
    "        acc.append('{0:.1f}%'.format((accuracy)*100))\n",
    "        f1.append('{0:.3f}'. format(f1score2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated mean test scores of the 5 repeats per percentage randomly changed labels: Calculations were performed with excell because the append list function did not work like expected/wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_perc = [.296, .244, .179, .115, .089, .053, .0322, .016, .010, .005]\n",
    "acc_perc = [69.5, 69.2, 68.6, 68.0, 67.8, 67.6, 67.4, 67.2, 67.1, 67.1]\n",
    "spec_perc = [94.2, 95.8, 97.2, 98.3, 98.9, 99.6, 99.8, 99.9, 99.9, 100]\n",
    "sens_perc = [19.4, 15.1, 10.4, 6.3, 4.8, 2.7, 1.64, .82, .5, .26 ]\n",
    "perc = [5, 10, 15,20,25,30,35,40,45,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=4, sharex=True, figsize=(20,5))\n",
    "fig.suptitle('LR Model scores for randomly changing percentages of the labelled 1 observations', fontsize=20)\n",
    "\n",
    "ax.set_ylim(0,1)\n",
    "ax.plot(perc, f1_perc, 'b-o', label='f1-score')\n",
    "ax.hlines(y=f1score, xmin=0, xmax=50, linestyles='--', color='b', alpha=.5, label='f1 reference score')\n",
    "ax.set_xlabel('percentage of changed y_train labels', fontsize=16)\n",
    "ax.set_ylabel('average f1-score', fontsize=16)\n",
    "ax.legend(loc='upper right', fontsize=12)   \n",
    "\n",
    "ax1.set_ylim(0, 105)\n",
    "ax1.plot(perc, acc_perc, 'g-o', label='accuracy')\n",
    "ax1.hlines(y=((accuracy_1)*100), xmin=0, xmax=50, linestyles='--', color='g', alpha=.5, label='f1 reference score')\n",
    "ax1.set_xlabel('percentage of changed y_train labels', fontsize=16)\n",
    "ax1.set_ylabel('average accuracy score', fontsize=16)\n",
    "ax1.legend(loc='lower right', fontsize=12) \n",
    "\n",
    "ax2.set_ylim(0, 105)\n",
    "ax2.plot(perc, spec_perc, 'r-o', label='specificity')\n",
    "ax2.hlines(y=((recall_spec)*100), xmin=0, xmax=50, linestyles='--', color='r', alpha=.5, label='f1 reference score')\n",
    "ax2.set_xlabel('percentage of changed y_train labels', fontsize=16)\n",
    "ax2.set_ylabel('average specificity score', fontsize=16)\n",
    "ax2.legend(loc='lower right', fontsize=12) \n",
    "\n",
    "ax3.set_ylim(0, 105)\n",
    "ax3.plot(perc, sens_perc, 'y-o', label='sensitivity')\n",
    "ax3.hlines(y=((recall_sens)*100), xmin=0, xmax=50, linestyles='--', color='y', alpha=.5, label='f1 reference score')\n",
    "ax3.set_xlabel('percentage of changed y_train labels', fontsize=16)\n",
    "ax3.set_ylabel('average sensitivity score', fontsize=16)\n",
    "ax3.legend(loc='upper right', fontsize=12) \n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "#dump(model, 'filename.joblib')\n",
    "\n",
    "# pickled model loading via:\n",
    "# model = load('filename.joblib')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e9d5bc61b74a9209ecc27603fd0fafd08f846842740ad327b1e348c2723f88a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('baseupdated')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
